# 第一章 绪论
## 1.1

## 1.2
首先是一组概念辨析，
| 概念         | 描述                                                                 | 示例                                                                 |
|--------------|--------------------------------------------------------------------|--------------------------------------------------------------------|
| 数据集 (Data Set) | 一组记录的集合，每条记录是一个事件或对象的描述。                           | 包含多个西瓜的记录的集合。                                           |
| 示例/样本 (Instance/Sample) | 数据集中的一条记录，描述一个事件或对象。                                 | 一个西瓜的描述。                                                     |
| 属性/特征 (Attribute/Feature) | 反映事件或对象在某方面的表现或性质的事项。                               | 色泽、根蒂、敲声。                                                   |
| 属性值 (Attribute Value) | 属性上的具体取值。                                                     | 色泽的取值可以是“青绿”或“乌黑”。                                      |
| 属性空间/样本空间/输入空间 (Attribute Space/Sample Space/Input Space) | 由属性张成的空间，每个示例在该空间中有一个坐标位置。                     | 由“色泽”、“根蒂”、“敲声”三个属性张成的三维空间。                      |
| 特征向量 (Feature Vector) | 在属性空间中的一个点，表示一个示例的属性值组成的向量。                     | 一个西瓜的特征向量可以是 (青绿, 硬挺, 浊响)。                         |

具体来说，

| 示例 | 色泽 | 根蒂 | 敲声 |
|------|------|------|------|
| 西瓜1 | 青绿 | 硬挺 | 浊响 |
| 西瓜2 | 乌黑 | 稍蜷 | 沉闷 |
| 西瓜3 | 浅白 | 蜷缩 | 清脆 |
- **数据集** 是包含三个西瓜的集合。
- **示例/样本** 是每一行的数据，例如“西瓜1”。
- **属性/特征** 是每一列的名称，例如“色泽”。
- **属性值** 是每个单元格中的值，例如“青绿”。
- **属性空间** 是由“色泽”、“根蒂”、“敲声”三个属性张成的三维空间。
- **特征向量** 是每一行的数据组成的向量，例如“西瓜1”的特征向量是 (青绿, 硬挺, 浊响)。

有了数据集，就需要得到样本的“结果”信息（label）来进行学习和预测。

关于机器学习的分类：

| 离散                | 连续            |
| ------------------- | --------------- |
| 分类/classification | 回归/regression |

簇/cluster是对数据集分组形成的聚类，有无预习提供的标记对应了不同的学习任务。

- 有标签（监督学习）：离散（分类），连续（回归）
- 无标签（无监督学习）：聚类

**数据集的质量非常重要**，训练样本尽量多，且独立同分布的采样，才能学习出泛化能力强的模型。


## 1.3

哲学上，数学的方法论：演绎推理法；自然科学的方法论：总结归纳法。

- 假设空间：假设空间是所有可能的模型（或假设）的集合。在机器学习中，假设空间包含了所有可能的函数或模型，这些函数或模型可以用来对数据进行分类或回归等任务。例如，如果我们正在处理一个二分类问题，假设空间可能包括所有可能的决策树、线性分类器、神经网络等。
- 版本空间：版本空间是假设空间的一个子集，它包含了所有与训练数据一致的假设。也就是说，版本空间中的每个假设都能正确地拟合训练数据。版本空间的大小和形状取决于训练数据的质量和数量。随着训练数据的增加，版本空间会逐渐缩小，因为更多的数据会进一步约束假设空间。
- 关系：版本空间是假设空间的一个子集。假设空间是所有可能的模型的集合，而版本空间是从假设空间中筛选出来的、与训练数据一致的模型的集合。版本空间的大小和形状受到训练数据的直接影响，训练数据越多、质量越高，版本空间就越小，模型的泛化能力通常也越好。
- 学习的过程就是从假设空间寻找版本空间，并寻求缩小版本空间——真相。

## 1.4
![image](https://github.com/user-attachments/assets/d909e3fd-cb80-4037-b75f-011124f6ccde)
### 归纳偏好（Inductive Bias）
#### 通俗解释：
归纳偏好是指机器学习算法在学习过程中对某些假设或模型的“偏好”。换句话说，它是算法在学习时自带的一种“倾向性”或“偏见”，用来帮助它从数据中找到规律。

#### 举个例子：
假设你教一个小朋友识别动物。你告诉他：“所有会飞的动物都是鸟。”这个规则就是你的“归纳偏好”。虽然这个规则在大多数情况下是对的（比如鸽子、老鹰），但它也会出错（比如蝙蝠会飞但不是鸟）。这个偏好帮助小朋友快速学习，但也可能让他犯一些错误。

#### 在机器学习中：
- 线性模型偏好“数据是线性的”这种假设。
- 决策树偏好“数据可以通过分层规则划分”这种假设。
- 神经网络偏好“数据可以通过复杂的非线性关系建模”这种假设。

#### 总结：
归纳偏好是算法自带的一种“假设”或“倾向性”，它帮助算法从数据中学习，但也可能限制算法的能力。

### 没有免费午餐定理（No Free Lunch Theorem, NFL）
#### 通俗解释：
没有免费午餐定理的意思是，没有任何一种算法在所有问题上都表现最好。换句话说，如果一个算法在某些问题上表现很好，那它在其他问题上一定会表现得很差。

#### 举个例子：
假设你有一把万能钥匙，可以打开很多锁。这把钥匙在某些锁上非常好用，但在另一些锁上可能完全打不开。没有一把钥匙能打开所有的锁，这就是“没有免费午餐”的道理。

#### 在机器学习中：

如果你用一个算法（比如神经网络）在图像分类上表现很好，那它可能在文本分类上表现得很差。

如果你用一个算法（比如决策树）在结构化数据上表现很好，那它可能在处理复杂非线性数据时表现得很差。

#### 总结：
没有免费午餐定理告诉我们，不存在一种通用的、适用于所有问题的完美算法。选择算法时，需要根据具体问题的特点来决定。

### 归纳偏好 vs. 没有免费午餐定理
- 归纳偏好：算法自带的一种“偏好”或“假设”，帮助它学习数据中的规律。
- 没有免费午餐定理：没有任何算法能在所有问题上都表现最好，算法的表现取决于具体问题。

### 关系：
归纳偏好是算法的“个性”，而没有免费午餐定理告诉我们，算法的“个性”决定了它在某些问题上表现好，而在其他问题上表现差。

因此，选择算法时需要根据问题的特点来匹配算法的偏好。

## 1.5

## 1.6

## 1.7


