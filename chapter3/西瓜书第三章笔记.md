# 第三章 线性模型
## 3.1 基本形式
本章与传统统计学中的线性模型有高度重复之处，区别是传统统计学更看重假设检验的内容，机器学习中则引入了诸多新方法。
基本形式为，其特点是具有较好的可解释性：

$$
f(x) = w^{T}x+b
$$

## 3.2 线性回归

- 对于离散值，可以进行连续化，类似于计量经济学中的dummy variable
- 线性回归的解就是OLS的求解，书上给出的解与正规方程组等价
- 对于多元线性回归，其解法与统计学相同，将矩阵写成内积形式后展开并求导，其结果如下

![图片](https://github.com/user-attachments/assets/0b0dcef0-333d-4d08-b087-63c32748fb1e)

- 如果矩阵不满秩，则可以引入正则化（岭回归加入系数平方和，lasso加入系数绝对值，弹性网络都加入）
- GLM：将线性模型拓展为非线性模型，对于对数线性模型来说，其含义为x增长1单位，y增长1%（回顾计量经济学内容）

## 3.3 对数几率回归

基本思想：将回归转化为分类，实值转化为0/1值
使用函数：Sigmoid，其推导与含义如下

![image](https://github.com/user-attachments/assets/fd86265c-0f77-48db-9c19-0166d8571a17)

实际上就是用线性回归来预测真实的标记的**对数几率**，在实际中还需要设定决策边界来划分正反例。并且该方法是直接对分类可能性进行建模，无需事先假设数据分布，这样就避免了假设分布不准确所带来的问题。

在课本上还讲解了确定回归参数的方法，具体思路为：

- 将y视为后验概率估计后，求解MLE，可以得到如下结果。需要注意的是虽然求解MLE过程本身很简单，但是需要注意其细节

![image](https://github.com/user-attachments/assets/c15bb5d3-80aa-4005-a1c2-9f94f190474e)

- 该函数为凸（可以用海塞矩阵证明），可以用最优化理论解决（此处具体由最优化理论解决，不做深入探讨）

## 3.4 线性判别分析（LDA）

举个例子：
假设我们有一组学生的身高和体重数据，标签是性别（男/女）。LDA的目标是找到一条直线，将男生和女生的数据投影到这条直线上后：
- 男生的投影点集中在直线的一端
- 女生的投影点集中在直线的另一端
- 两类数据之间的间隔尽可能大，同时每类内部的点尽可能紧凑
这就是LDA，其图示如下：

![image](https://github.com/user-attachments/assets/8ee8a05c-59b7-471c-8f53-ff7a54663e15)

LDA的核心目标：最大化类间散度（不同类别之间的距离），同时最小化类内散度（同一类别的数据分散程度）。
公式推导如下：

![image](https://github.com/user-attachments/assets/f2698f6e-0d57-42fb-9101-5252911a5233)

![image](https://github.com/user-attachments/assets/2c56f364-9ce1-4866-b0c4-4e8a78865168)

此处需要注意的是，图中所给解法未考虑限制条件，故直接求偏导，而书中给了一个一般化的条件，使用拉格朗日法求解，两者都可以。

对于K个问题的推广如下：

![image](https://github.com/user-attachments/assets/2156f819-1375-42a7-859d-fc42d3ad8c2e)

## 3.5 多分类学习

核心：拆解法，把多分类拆成多个二分类：一对一、一对多、多对多，下图很直观的展示了步骤

![image](https://github.com/user-attachments/assets/bf719465-8a68-4196-aa79-a22427add780)

一对一相比起一对多来说，存储开销更大，但是单次训练的时间开销更小，所以类别很多时一对一总时间更小。

多对多需要特定的方法，比如纠错输出码（ECOC）

### 通俗解释ECOC

纠错输出码（Error-Correcting Output Codes, ECOC） 是一种让机器学习模型更鲁棒地解决多分类问题的技术。它的核心思想是：通过设计一种“编码-解码”策略，将复杂的多分类问题拆解成多个简单的二分类问题，并利用冗余信息自动纠正预测中的错误。

#### 核心思想类比
想象你要做一道多选题，正确答案是选项C，但你有以下策略：

- 拆解问题：将多选题拆成多个判断题（例如：“选项A对吗？”“选项B对吗？”...）。

- 冗余设计：每个判断题由多个不同的“小裁判”独立回答。

- 纠错机制：即使某个小裁判答错了，综合多数裁判的结果仍能选出正确答案。

ECOC就是类似的思路，但用数学编码来实现。

#### 分步解释
##### 1. 编码阶段（设计“密码本”）
问题：假设要分类猫、狗、鸟三类。

设计编码矩阵：为每个类别分配一个独特的“二进制码”，例如：

类别	编码（分类器1, 分类器2, 分类器3）
猫	1   1   0
狗	0   1   1
鸟	1   0   1
每个分类器的任务被定义为：

- 分类器1：区分“猫 vs 非猫”

- 分类器2：区分“狗 vs 非狗”

- 分类器3：区分“鸟 vs 非鸟”

##### 2. 训练阶段
训练多个二分类器（如SVM、决策树），每个分类器对应编码矩阵中的一列。

例如：

- 分类器1：所有“猫”样本标记为1，其他标记为0；

- 分类器2：所有“狗”样本标记为1，其他标记为0；

- 分类器3：所有“鸟”样本标记为1，其他标记为0。

##### 3. 预测阶段（解码 + 纠错）
- 步骤1：用所有分类器对新样本预测，得到一个二进制码（如 1 0 1）。

- 步骤2：将这个预测码与编码矩阵中每个类别的码对比，找到最接近的码。

纠错原理：如果某个分类器预测错误（例如输出码为 1 0 0），通过对比发现：

猫的码 1 1 0 → 距离为1（错1位）

狗的码 0 1 1 → 距离为3（错3位）

鸟的码 1 0 1 → 距离为1（错1位）

最终预测：选距离最小的猫或鸟（可能需要其他策略打破平局）。

#### 关键优势
- 纠错能力：允许部分分类器出错，仍能通过冗余设计纠正结果。

- 灵活性：支持任意多分类问题（尤其类别数多时优势明显）。

- 兼容性：可与任何二分类模型（如SVM、神经网络）结合使用。

**对于更精细的数学细节此处略去**

## 3.6 类别不平衡问题

### 再缩放:分类器的预测几率高于观测几率就应判定为正例

弊端：采样未必无偏，则解决方法有：

- 欠采样：去除一部分使之平衡（EasyEnsemble将反例划分为若干个集合供不同学习器使用，这样对每个学习器来看都进行了欠采样）
- 过采样：增加一部分使之平衡（SMOTE法通过插值获得）
- 阈值移动：使用以下公式进行预测（其中m代表正反例个数）

![image](https://github.com/user-attachments/assets/724c6282-929c-4f94-9236-7510b0540619)


